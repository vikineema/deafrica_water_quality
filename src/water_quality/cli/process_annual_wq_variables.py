import gc
import json
import os
import sys
import warnings
from itertools import chain

import click
import numpy as np
import toolz
import xarray as xr
from datacube import Datacube
from deafrica_tools.dask import create_local_dask_cluster
from odc import dscache
from odc.geo.xr import write_cog
from odc.stats.model import DateTimeRange

from water_quality.io import (
    check_directory_exists,
    check_file_exists,
    get_filesystem,
    get_parent_dir,
    get_wq_cog_url,
    get_wq_csv_url,
    join_url,
)
from water_quality.logs import setup_logging
from water_quality.mapping.algorithms import (
    WQ_vars,
    geomedian_FAI,
    geomedian_NDVI,
    normalise_and_stack_wq_vars,
)
from water_quality.mapping.hue import geomedian_hue
from water_quality.mapping.instruments import (
    INSTRUMENTS_PRODUCTS,
    check_instrument_dates,
    get_instruments_list,
)
from water_quality.mapping.load_data import build_wq_agm_dataset
from water_quality.mapping.pixel_correction import R_correction
from water_quality.mapping.water_detection import water_analysis
from water_quality.metadata.prepare_metadata import prepare_dataset
from water_quality.tasks import create_task_id, parse_task_id, split_tasks


def setup_dask_if_needed():
    """Start local Dask cluster in Sandbox, else return None."""
    if bool(os.environ.get("JUPYTERHUB_USER", None)):
        return create_local_dask_cluster(
            display_client=False, return_client=True
        )
    return None


@click.command(
    name="process-annual-wq-variables",
    no_args_is_help=True,
)
@click.option(
    "--tasks",
    help="List of comma separated tasks in the format "
    "period/x{x:02d}/y{y:02d} to generate water quality variables for. "
    "For example `2015--P1Y/x200/y034,2015--P1Y/x178/y095,2015--P1Y/x199/y100`",
)
@click.argument(
    "cache-file-path",
    type=str,
)
@click.argument(
    "output-directory",
    type=str,
)
@click.argument(
    "max-parallel-steps",
    type=int,
)
@click.argument(
    "worker-idx",
    type=int,
)
@click.option(
    "--overwrite/--no-overwrite",
    default=False,
    show_default=True,
    help=(
        "If overwrite is True tasks that have already been processed "
        "will be rerun. "
    ),
)
def cli(
    tasks: str,
    cache_file_path: str,
    output_directory: str,
    max_parallel_steps: int,
    worker_idx: int,
    overwrite: bool,
):
    """
    Get the annual Water Quality variables for the input tasks and write
    the resulting water quality variables to COG files.

    CACHE_FILE_PATH: Path to the file database containing the tasks and
    datasets to be processed. Generated by `wq-generate-tasks`.

    OUTPUT_DIRECTORY: The directory to write the water quality variables
    COG files to for each task.

    MAX_PARALLEL_STEPS: The total number of parallel workers or pods
    expected in the workflow. This value is used to divide the list of
    tasks to be processed among the available workers.

    WORKER_IDX: The sequential index (0-indexed) of the current worker.
    This index determines which subset of tasks the current worker will
    process.
    """
    log = setup_logging()

    cache = dscache.open_ro(cache_file_path)

    # Load the configuration
    # This should have been validated during task generation.
    config = cache.get_info_dict("wq_config")
    # resolution = config["resolution"]
    WFTH = config["WFTH"]
    # WFTL = config["WFTL"]
    # PWT = config["PWT"]
    # SC = config["SC"]
    product_name = config["product_name"]
    product_version = config["product_version"]
    # Instruments to use should also have been validated
    # during task generation.
    instruments_to_use = config["instruments_to_use"]

    # Grid properties
    grid_name = config["grid_name"]
    grid_spec = cache.grids[grid_name]

    input_products = {
        v: k for k, vals in INSTRUMENTS_PRODUCTS.items() for v in vals
    }

    # Load all tasks and split for this worker
    all_task_ids = sorted([i[0] for i in cache.tiles(grid_name)])

    if tasks:
        task_filter = [i.strip() for i in tasks.split(",")]
        # "period/x{x:02d}/y{y:02d}" to (period, x, y)
        task_filter = [parse_task_id(i) for i in task_filter]
        found_tasks = list(set(task_filter).intersection(set(all_task_ids)))

        if len(found_tasks) == 0:
            log.error(
                "No matching tasks found in the file database "
                f"for the provided filter: {tasks}."
            )
            sys.exit(1)
        else:
            log.info(
                f"Found {len(found_tasks)} matching tasks "
                f"in the file database for the provided filter: {tasks}."
            )
        tasks_to_run = split_tasks(found_tasks, max_parallel_steps, worker_idx)
    else:
        tasks_to_run = split_tasks(
            all_task_ids, max_parallel_steps, worker_idx
        )

    if not tasks_to_run:
        log.warning(f"Worker {worker_idx} has no tasks to process. Exiting.")
        sys.exit(0)

    log.info(f"Worker {worker_idx} processing {len(tasks_to_run)} tasks")

    dc = Datacube(app="ProcessAnnualWQVariables")

    failed_tasks = []
    # Process each task
    for idx, task_id in enumerate(tasks_to_run):
        # Parse task information
        temporal_id, tile_idx, tile_idy = task_id
        tile_id = (tile_idx, tile_idy)
        task_id_str = create_task_id(temporal_id, tile_id)
        log.info(
            f"Processing task {idx + 1} of {len(tasks_to_run)}: {task_id_str} "
        )
        try:
            temporal_range = DateTimeRange(temporal_id)
            start_date = temporal_range.start.strftime("%Y-%m-%d")
            end_date = temporal_range.end.strftime("%Y-%m-%d")
            check_instrument_dates(
                instruments_to_use, start_date, end_date, raise_errors=True
            )

            # Check if task already processed
            if not overwrite:
                output_csv_url = get_wq_csv_url(
                    output_directory=output_directory,
                    tile_id=tile_id,
                    temporal_id=temporal_id,
                    product_name=product_name,
                    product_version=product_version,
                )
                if check_file_exists(output_csv_url):
                    log.info(
                        f"Task {task_id_str} already processed. Skipping."
                    )
                    continue

            dss = list(cache.stream_grid_tile(task_id, grid_name))
            dss = toolz.groupby(
                lambda ds: input_products[ds.product.name], dss
            )
            # Setup Dask if needed
            client = setup_dask_if_needed()

            # Load data
            tile_geobox = grid_spec.tile_geobox(tile_index=tile_id)
            ds = build_wq_agm_dataset(
                datasets=dss, tile_geobox=tile_geobox, dc=dc
            )

            # Close Dask client if it was created
            if client is not None:
                client.close()

            ds = water_analysis(
                ds,
                water_frequency_threshold=WFTH,
            )

            # Floating Algea Index
            ds = geomedian_FAI(ds)

            # NDVI
            ds = geomedian_NDVI(ds)

            # Set the clear water mask
            ds["clearwater"] = xr.where(
                np.isnan(ds.agm_fai),
                xr.where(ds.water_mask == 1, True, False),
                False,
            )

            # Reflectance correction
            ds = R_correction(ds, instruments_to_use, drop=False)

            # Hue calculation
            ds = geomedian_hue(ds)

            # OWT calculation
            # Turned off OWT until PR#25 is merged
            # TODO: turn on once PR#25 is merged
            # OWT calculation
            # ds = run_OWT(ds)

            # Run WQ algorithms
            log.info("Applying the WQ algorithms to water areas.")
            instruments_list = get_instruments_list(instruments_to_use)
            ds_out, wq_vars_df = WQ_vars(
                ds,
                instruments_list=instruments_list,
                stack_wq_vars=False,
            )

            ds_out = normalise_and_stack_wq_vars(
                ds=ds_out,
                wq_vars_table=wq_vars_df,
                water_frequency_threshold=0,
            )
            gc.collect()

            # Get list of WQ variables
            wq_vars_list = list(
                chain.from_iterable(
                    [
                        wq_vars_df[col].dropna().to_list()
                        for col in wq_vars_df.columns
                    ]
                )
            )

            # TODO: Refine list of expected water quality variables
            # to keep in final output dataset.
            initial_keep_list = [
                # water_analysis
                "wofs_ann_freq_sigma",
                "wofs_ann_confidence",
                "wofs_pw_threshold",
                "wofs_ann_pwater",
                "wofs_ann_water",
                "wofs_ann_watermask",
                # FAI
                "agm_fai",
                "msi_agm_fai",
                "oli_agm_fai",
                "tm_agm_fai",
                # NDVI
                "agm_ndvi",
                "msi_agm_ndvi",
                "oli_agm_ndvi",
                "tm_agm_ndvi",
                # Clear water mask
                "clearwater",
                # Hue
                "agm_hue",
                "msi_agm_hue",
                "oli_agm_hue",
                "tm_agm_hue",
                # optical water type
                "owt_msi",
                "owt_oli",
                # wq variables
                "tss",
                "chla",
                "tsi",
            ]

            # Create drop list for unused variables
            droplist = []
            for instrument in list(instruments_list.keys()):
                for band in list(instruments_list[instrument].keys()):
                    variable = instruments_list[instrument][band]["varname"]
                    if variable not in initial_keep_list:
                        droplist = np.append(droplist, variable)
                        droplist = np.append(droplist, variable + "r")

            ds_out = ds_out.drop_vars(droplist, errors="ignore")

            # Save each band as COG
            fs = get_filesystem(output_directory, anon=False)
            bands = list(ds_out.data_vars)

            for band in bands:
                output_cog_url = get_wq_cog_url(
                    output_directory=output_directory,
                    tile_id=tile_id,
                    temporal_id=temporal_id,
                    band_name=band,
                    product_name=product_name,
                    product_version=product_version,
                )

                # Enforce data type for all bands to float32
                da: xr.DataArray = ds_out[band].astype(np.float32)

                # Set attributes
                if band not in wq_vars_list:
                    da.attrs = dict(
                        nodata=np.nan,
                        scales=1,
                        offsets=0,
                        product_name=product_name,
                        product_version=product_version,
                    )
                else:
                    da.attrs.update(
                        dict(
                            product_name=product_name,
                            product_version=product_version,
                        )
                    )

                # Write COG
                cog_bytes = write_cog(
                    geo_im=da,
                    fname=":mem:",
                    overwrite=True,
                    nodata=da.attrs["nodata"],
                    tags=da.attrs,
                )
                with fs.open(output_cog_url, "wb") as f:
                    f.write(cog_bytes)
                log.info(f"Band {band} saved to {output_cog_url}")

            # Save WQ parameters table
            output_csv_url = get_wq_csv_url(
                output_directory=output_directory,
                tile_id=tile_id,
                temporal_id=temporal_id,
                product_name=product_name,
                product_version=product_version,
            )
            with fs.open(output_csv_url, mode="w") as f:
                wq_vars_df.to_csv(f, index=False)

            # Generate STAC metadata
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)
                log.info("Creating metadata STAC file ...")
                stac_file_url = prepare_dataset(
                    dataset_path=get_parent_dir(output_csv_url),
                    #  source_datasets_uuids=source_datasets_uuids,
                )

            log.info(f"Successfully processed task: {task_id_str}")

        except Exception as error:
            log.exception(error)
            failed_tasks.append(task_id)

    cache.close()

    # Handle failed tasks
    if failed_tasks:
        failed_tasks_json_array = json.dumps(failed_tasks)

        tasks_directory = "/tmp/"
        failed_tasks_output_file = join_url(tasks_directory, "failed_tasks")

        fs = get_filesystem(path=tasks_directory, anon=False)
        if not check_directory_exists(path=tasks_directory):
            fs.mkdirs(path=tasks_directory, exist_ok=True)

        with fs.open(failed_tasks_output_file, "a") as file:
            file.write(failed_tasks_json_array + "\n")
        log.error(f"Failed tasks: {failed_tasks_json_array}")
        log.info(f"Failed tasks written to {failed_tasks_output_file}")
        sys.exit(1)
    else:
        log.info(f"Worker {worker_idx} completed successfully!")
        sys.exit(0)


if __name__ == "__main__":
    cli()
