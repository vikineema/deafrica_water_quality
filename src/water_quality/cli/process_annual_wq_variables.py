import gc
import json
import os
import sys
import warnings

import click
import numpy as np
import toolz
import xarray as xr
from datacube import Datacube
from deafrica_tools.dask import create_local_dask_cluster
from odc import dscache
from odc.geo.xr import write_cog
from odc.stats.model import DateTimeRange

from water_quality.io import (
    check_directory_exists,
    get_filesystem,
    get_parent_dir,
    get_wq_cog_url,
    get_wq_dataset_path,
    get_wq_stac_url,
    join_url,
)
from water_quality.logs import setup_logging
from water_quality.mapping.instruments import (
    INSTRUMENTS_PRODUCTS,
    check_instrument_dates,
    get_instruments_list,
)
from water_quality.mapping.water_detection import load_5year_water_mask
from water_quality.metadata.prepare_metadata import prepare_dataset
from water_quality.tasks import create_task_id, parse_task_id, split_tasks


def setup_dask_if_needed():
    """Start local Dask cluster in Sandbox, else return None."""
    if bool(os.environ.get("JUPYTERHUB_USER", None)):
        return create_local_dask_cluster(
            display_client=False, return_client=True
        )
    return None


@click.command(
    name="process-annual-wq-variables",
    no_args_is_help=True,
)
@click.option(
    "--tasks",
    help="List of comma separated tasks in the format "
    "period/x{x:02d}/y{y:02d} to generate water quality variables for. "
    "For example `2015--P1Y/x200/y034,2015--P1Y/x178/y095,2015--P1Y/x199/y100`",
)
@click.argument(
    "cache-file-path",
    type=str,
)
@click.argument(
    "output-directory",
    type=str,
)
@click.argument(
    "max-parallel-steps",
    type=int,
)
@click.argument(
    "worker-idx",
    type=int,
)
@click.option(
    "--overwrite/--no-overwrite",
    default=False,
    show_default=True,
    help=(
        "If overwrite is True tasks that have already been processed "
        "will be rerun. "
    ),
)
def cli(
    tasks: str,
    cache_file_path: str,
    output_directory: str,
    max_parallel_steps: int,
    worker_idx: int,
    overwrite: bool,
):
    """
    Get the annual Water Quality variables for the input tasks and write
    the resulting water quality variables to COG files.

    CACHE_FILE_PATH: Path to the file database containing the tasks and
    datasets to be processed. Generated by `wq-generate-tasks`.

    OUTPUT_DIRECTORY: The directory to write the water quality variables
    COG files to for each task.

    MAX_PARALLEL_STEPS: The total number of parallel workers or pods
    expected in the workflow. This value is used to divide the list of
    tasks to be processed among the available workers.

    WORKER_IDX: The sequential index (0-indexed) of the current worker.
    This index determines which subset of tasks the current worker will
    process.
    """
    log = setup_logging()

    cache = dscache.open_ro(cache_file_path)

    # Load the configuration
    # This should have been validated during task generation.
    config = cache.get_info_dict("wq_config")
    # resolution = config["resolution"]
    # WFTH = config["WFTH"]
    # WFTL = config["WFTL"]
    # PWT = config["PWT"]
    # SC = config["SC"]
    product_name = config["product_name"]
    product_version = config["product_version"]
    # Instruments to use should also have been validated
    # during task generation.
    cfg_instruments_to_use = config["instruments_to_use"]

    # Grid properties
    grid_name = config["grid_name"]
    grid_spec = cache.grids[grid_name]

    input_products = {
        v: k for k, vals in INSTRUMENTS_PRODUCTS.items() for v in vals
    }

    # Load all tasks and split for this worker
    all_task_ids = sorted([i[0] for i in cache.tiles(grid_name)])

    if tasks:
        task_filter = [i.strip() for i in tasks.split(",")]
        # "period/x{x:02d}/y{y:02d}" to (period, x, y)
        task_filter = [parse_task_id(i) for i in task_filter]
        found_tasks = list(set(task_filter).intersection(set(all_task_ids)))

        if len(found_tasks) == 0:
            log.error(
                "No matching tasks found in the file database "
                f"for the provided filter: {tasks}."
            )
            sys.exit(1)
        else:
            log.info(
                f"Found {len(found_tasks)} matching tasks "
                f"in the file database for the provided filter: {tasks}."
            )
        tasks_to_run = split_tasks(found_tasks, max_parallel_steps, worker_idx)
    else:
        tasks_to_run = split_tasks(
            all_task_ids, max_parallel_steps, worker_idx
        )

    if not tasks_to_run:
        log.warning(f"Worker {worker_idx} has no tasks to process. Exiting.")
        sys.exit(0)

    log.info(f"Worker {worker_idx} processing {len(tasks_to_run)} tasks")

    dc = Datacube(app="ProcessAnnualWQVariables")

    failed_tasks = []
    # Process each task
    for idx, task_id in enumerate(tasks_to_run):
        # Parse task information
        temporal_id, tile_idx, tile_idy = task_id
        tile_id = (tile_idx, tile_idy)
        task_id_str = create_task_id(temporal_id, tile_id)
        log.info(
            f"Processing task {idx + 1} of {len(tasks_to_run)}: {task_id_str} "
        )
        try:
            temporal_range = DateTimeRange(temporal_id)
            start_date = temporal_range.start.strftime("%Y-%m-%d")
            end_date = temporal_range.end.strftime("%Y-%m-%d")
            instruments_to_use = check_instrument_dates(
                cfg_instruments_to_use,
                start_date,
                end_date,
                raise_errors=False,
            )
            instruments_list = get_instruments_list(instruments_to_use)

            expected_stac_path = get_wq_stac_url(
                get_wq_dataset_path(
                    output_directory=output_directory,
                    tile_id=tile_id,
                    temporal_id=temporal_id,
                    product_name=product_name,
                    product_version=product_version,
                )
            )
            exists = check_directory_exists(expected_stac_path)
            if exists and not overwrite:
                log.info(
                    f"STAC file already exists for task {task_id_str} at "
                    f"{expected_stac_path} and overwrite is False. "
                    "Skipping processing for this task."
                )
                continue

            dss = list(cache.stream_grid_tile(task_id, grid_name))
            dss = toolz.groupby(
                lambda ds: input_products[ds.product.name], dss
            )
            tile_geobox = grid_spec.tile_geobox(tile_index=tile_id)

            # wq_ds = {}
            wq_ds = xr.Dataset()

            # Generate 5 year water mask
            wq_ds["water_mask"] = load_5year_water_mask(
                dss=dss,
                tile_geobox=tile_geobox,
                compute=True,
                dc=dc,
            )
            gc.collect()

            # bands = list(wq_ds.keys())
            bands = list(wq_ds.data_vars)

            is_empty = all([wq_ds[band].size == 0 for band in bands])
            if is_empty:
                log.error(
                    f"No water quality variables generated for task {task_id_str}. "
                )
                continue

            # Save each band as COG
            fs = get_filesystem(output_directory, anon=False)
            for band in bands:
                da = wq_ds[band]
                if da.size == 0:
                    continue
                # Set attributes
                if "nodata" not in list(da.attrs.keys()):
                    da.attrs = dict(
                        nodata=np.nan,
                        scales=1,
                        offsets=0,
                        product_name=product_name,
                        product_version=product_version,
                    )
                else:
                    da.attrs.update(
                        dict(
                            product_name=product_name,
                            product_version=product_version,
                        )
                    )

                # Write COG
                cog_bytes = write_cog(
                    geo_im=da,
                    fname=":mem:",
                    overwrite=True,
                    nodata=da.attrs["nodata"],
                    tags=da.attrs,
                )

                output_cog_url = get_wq_cog_url(
                    output_directory=output_directory,
                    tile_id=tile_id,
                    temporal_id=temporal_id,
                    band_name=band,
                    product_name=product_name,
                    product_version=product_version,
                )
                with fs.open(output_cog_url, "wb") as f:
                    f.write(cog_bytes)
                log.info(f"Band {band} saved to {output_cog_url}")

            # Generate STAC metadata
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)
                log.info("Creating metadata STAC file ...")
                stac_file_url = prepare_dataset(  # noqa F841
                    dataset_path=get_parent_dir(output_cog_url),
                )

            log.info(f"Successfully processed task: {task_id_str}")

        except Exception as error:
            log.exception(error)
            failed_tasks.append(task_id)

    cache.close()

    # Handle failed tasks
    if failed_tasks:
        failed_tasks_json_array = json.dumps(failed_tasks)

        tasks_directory = "/tmp/"
        failed_tasks_output_file = join_url(tasks_directory, "failed_tasks")

        fs = get_filesystem(path=tasks_directory, anon=False)
        if not check_directory_exists(path=tasks_directory):
            fs.mkdirs(path=tasks_directory, exist_ok=True)

        with fs.open(failed_tasks_output_file, "a") as file:
            file.write(failed_tasks_json_array + "\n")
        log.error(f"Failed tasks: {failed_tasks_json_array}")
        log.info(f"Failed tasks written to {failed_tasks_output_file}")
        sys.exit(1)
    else:
        log.info(f"Worker {worker_idx} completed successfully!")
        sys.exit(0)


if __name__ == "__main__":
    cli()
