import gc
import json
import os
import sys
import warnings

import click
import numpy as np
import toolz
import xarray as xr
from datacube import Datacube
from deafrica_tools.dask import create_local_dask_cluster
from odc import dscache
from odc.geo.xr import write_cog
from odc.stats.model import DateTimeRange

from water_quality.io import (
    check_directory_exists,
    get_filesystem,
    get_parent_dir,
    get_wq_cog_url,
    get_wq_csv_url,
    get_wq_dataset_path,
    get_wq_stac_url,
    is_s3_path,
    join_url,
)
from water_quality.logs import setup_logging
from water_quality.mapping.algorithms import WQ_vars
from water_quality.mapping.fai import geomedian_FAI
from water_quality.mapping.hue import geomedian_hue
from water_quality.mapping.instruments import (
    INSTRUMENTS_PRODUCTS,
    check_instrument_dates,
)
from water_quality.mapping.load_data import load_annual_data
from water_quality.mapping.ndvi import geomedian_NDVI
from water_quality.mapping.optical_water_type import run_OWT
from water_quality.mapping.pixel_correction import apply_R_correction
from water_quality.mapping.water_detection import (
    clear_water_mask,
    five_year_water_mask,
)
from water_quality.metadata.prepare_metadata import prepare_dataset
from water_quality.tasks import (
    create_task_id,
    filter_tasks_by_task_id,
    filter_tasks_by_tile_id,
    split_tasks,
)


def setup_dask_if_needed():
    """Start local Dask cluster in Sandbox, else return None."""
    if bool(os.environ.get("JUPYTERHUB_USER", None)):
        return create_local_dask_cluster(
            display_client=False, return_client=True
        )
    return None


@click.command(
    name="process-annual-wq-variables",
    no_args_is_help=True,
)
@click.option(
    "--tasks",
    help="List of comma separated tasks in the format "
    "period/x{x:02d}/y{y:02d} to generate water quality variables for. "
    "For example `2015--P1Y/x200/y034,2015--P1Y/x178/y095,2015--P1Y/x199/y100`",
)
@click.option(
    "--tiles",
    help="List of comma separated tiles in the format "
    "x{x:02d}/y{y:02d} to generate water quality variables for. "
    "For example `x200/y034,x178/y095,x199/y100`",
)
@click.argument(
    "cache-file-path",
    type=str,
)
@click.argument(
    "output-directory",
    type=str,
)
@click.argument(
    "max-parallel-steps",
    type=int,
)
@click.argument(
    "worker-idx",
    type=int,
)
@click.option(
    "--overwrite/--no-overwrite",
    default=False,
    show_default=True,
    help=(
        "If overwrite is True tasks that have already been processed "
        "will be rerun. "
    ),
)
def cli(
    tasks: str,
    tiles: str,
    cache_file_path: str,
    output_directory: str,
    max_parallel_steps: int,
    worker_idx: int,
    overwrite: bool,
):
    """
    Get the annual Water Quality variables for the input tasks and write
    the resulting water quality variables to COG files.

    CACHE_FILE_PATH: Path to the file database containing the tasks and
    datasets to be processed. Generated by `wq-generate-tasks`.

    OUTPUT_DIRECTORY: The directory to write the water quality variables
    COG files to for each task.

    MAX_PARALLEL_STEPS: The total number of parallel workers or pods
    expected in the workflow. This value is used to divide the list of
    tasks to be processed among the available workers.

    WORKER_IDX: The sequential index (0-indexed) of the current worker.
    This index determines which subset of tasks the current worker will
    process.
    """
    log = setup_logging()

    if is_s3_path(cache_file_path):
        local_cache_file_path = "/tmp/cachedb.db"
        fs = get_filesystem(cache_file_path, anon=False)
        fs.get(cache_file_path, local_cache_file_path)
        log.info(
            f"Downloaded cache file from {cache_file_path} to {local_cache_file_path}"
        )
        cache = dscache.open_ro(local_cache_file_path)
    else:
        cache = dscache.open_ro(cache_file_path)

    # Load the configuration
    # This should have been validated during task generation.
    config = cache.get_info_dict("wq_config")
    # resolution = config["resolution"]
    WFTH = config["WFTH"]
    # WFTL = config["WFTL"]
    # PWT = config["PWT"]
    # SC = config["SC"]
    product_name = config["product_name"]
    product_version = config["product_version"]
    # Instruments to use should also have been validated
    # during task generation.
    cfg_instruments_to_use = config["instruments_to_use"]

    # Grid properties
    grid_name = config["grid_name"]
    grid_spec = cache.grids[grid_name]

    input_products = {
        v: k for k, vals in INSTRUMENTS_PRODUCTS.items() for v in vals
    }

    # Load all tasks and split for this worker
    all_task_ids = sorted([i[0] for i in cache.tiles(grid_name)])

    if tasks and tiles:
        raise ValueError("Use either tasks or tiles, not both.")
    else:
        if tasks or tiles:
            if tasks:
                filtered_tasks = filter_tasks_by_task_id(
                    all_task_ids=all_task_ids, task_ids=tasks
                )
                filter_str = tasks
            else:
                filtered_tasks = filter_tasks_by_tile_id(
                    all_task_ids=all_task_ids, tile_ids=tiles
                )
                filter_str = tiles

            if len(filtered_tasks) == 0:
                error_msg = f"No matching tasks found in the file database for the provided filter: {filter_str}."
                log.error(error_msg)
                sys.exit(1)
            else:
                log.info(
                    f"Found {len(filtered_tasks)} matching tasks "
                    f"in the file database for the provided filter: {filter_str}."
                )
                tasks_to_run = split_tasks(
                    filtered_tasks, max_parallel_steps, worker_idx
                )
        else:
            tasks_to_run = split_tasks(
                all_task_ids, max_parallel_steps, worker_idx
            )

    if not tasks_to_run:
        log.warning(f"Worker {worker_idx} has no tasks to process. Exiting.")
        sys.exit(0)

    log.info(f"Worker {worker_idx} processing {len(tasks_to_run)} tasks")

    dc = Datacube(app="ProcessAnnualWQVariables")

    failed_tasks = []
    # Process each task
    for idx, task_id in enumerate(tasks_to_run):
        # Parse task information
        temporal_id, tile_idx, tile_idy = task_id
        tile_id = (tile_idx, tile_idy)
        task_id_str = create_task_id(temporal_id, tile_id)
        log.info(
            f"Processing task {idx + 1} of {len(tasks_to_run)}: {task_id_str} "
        )
        try:
            temporal_range = DateTimeRange(temporal_id)
            start_date = temporal_range.start.strftime("%Y-%m-%d")
            end_date = temporal_range.end.strftime("%Y-%m-%d")
            instruments_to_use = check_instrument_dates(  # noqa F841
                cfg_instruments_to_use,
                start_date,
                end_date,
                raise_errors=False,
            )

            expected_stac_path = get_wq_stac_url(
                get_wq_dataset_path(
                    output_directory=output_directory,
                    tile_id=tile_id,
                    temporal_id=temporal_id,
                    product_name=product_name,
                    product_version=product_version,
                )
            )
            exists = check_directory_exists(expected_stac_path)
            if exists and not overwrite:
                log.info(
                    f"STAC file already exists for task {task_id_str} at "
                    f"{expected_stac_path} and overwrite is False. "
                    "Skipping processing for this task."
                )
                continue

            dss = list(cache.stream_grid_tile(task_id, grid_name))
            dss = toolz.groupby(
                lambda ds: input_products[ds.product.name], dss
            )
            tile_geobox = grid_spec.tile_geobox(tile_index=tile_id)

            # Load annual data for all instruments
            annual_data = load_annual_data(
                dss=dss,
                tile_geobox=tile_geobox,
                compute=False,
                dc=dc,
            )

            wq_ds = {}

            # Generate 5 year water mask
            wq_ds["water_mask"] = five_year_water_mask(
                annual_data=annual_data,
                compute=True,
            )
            gc.collect()

            # Calculate the Floating Algae Index (FAI) for the
            # available instruments
            wq_ds["fai"] = geomedian_FAI(
                annual_data=annual_data,
                water_mask=wq_ds["water_mask"],
                compute=True,
            )
            gc.collect()

            # Calculate the clear water mask.
            wq_ds["clear_water"] = clear_water_mask(
                annual_data=annual_data,
                water_frequency_threshold=WFTH,
                water_mask=wq_ds["water_mask"],
                agm_fai=wq_ds["fai"]["agm_fai"],
                compute=True,
            )
            gc.collect()

            # Calculate the Normalized Difference Vegetation Index (NDVI)
            # for the available instruments.
            wq_ds["ndvi"] = geomedian_NDVI(
                annual_data=annual_data,
                water_mask=wq_ds["water_mask"],
                compute=True,
            )
            gc.collect()

            # Apply Rayleigh correction to the available instruments.
            annual_data = apply_R_correction(
                instrument_data=annual_data,
                water_mask=wq_ds["water_mask"],
                compute=False,
                drop=True,
            )
            gc.collect()

            # Functions beyond this point do not make provisions for
            # uncorrected data i.e. the original band being present plus
            # the corrrected band (e.g. msi04_agm and msi04_agmr). Always
            # drop the uncorrected bands after applying the R correction.

            # Calculate the Hue for the available instruments.
            wq_ds["hue"] = geomedian_hue(
                annual_data=annual_data,
                clear_water_mask=wq_ds["clear_water"],
                compute=True,
            )
            gc.collect()

            # Run Optical Water Type classification
            wq_ds["owt"] = run_OWT(
                instrument_data=annual_data,
                clear_water_mask=wq_ds["clear_water"],
                compute=True,
            )
            gc.collect()

            # Calculate TSM, TSI and Chla Water Quality variables
            wq_ds["tsm_chla_tsi"], wq_vars_df = WQ_vars(
                annual_data=annual_data,
                water_mask=wq_ds["water_mask"],
                compute=True,
                stack_wq_vars=True,
            )
            gc.collect()

            measurement_paths: list[str] = []
            for wq_var_group in list(wq_ds.keys()):
                ds = wq_ds[wq_var_group]
                if isinstance(ds, xr.DataArray):
                    name = ds.name
                    ds = ds.to_dataset(name=name)

                # Save each band as COG
                fs = get_filesystem(output_directory, anon=False)
                bands = list(ds.data_vars.keys())
                for band in bands:
                    da = ds[band]
                    if da.size == 0:
                        continue
                    # Set attributes
                    if "scales" not in list(da.attrs.keys()):
                        da.attrs = dict(
                            nodata=np.nan,
                            scales=1,
                            offsets=0,
                            product_name=product_name,
                            product_version=product_version,
                        )
                    else:
                        da.attrs.update(
                            dict(
                                product_name=product_name,
                                product_version=product_version,
                            )
                        )

                    # Write COG
                    cog_bytes = write_cog(
                        geo_im=da,
                        fname=":mem:",
                        overwrite=True,
                        nodata=da.attrs["nodata"],
                        tags=da.attrs,
                    )

                    output_cog_url = get_wq_cog_url(
                        output_directory=output_directory,
                        tile_id=tile_id,
                        temporal_id=temporal_id,
                        band_name=band,
                        product_name=product_name,
                        product_version=product_version,
                    )
                    with fs.open(output_cog_url, "wb") as f:
                        f.write(cog_bytes)
                    log.info(f"Band {band} saved to {output_cog_url}")
                    measurement_paths.append(output_cog_url)

                if wq_var_group == "tsm_chla_tsi":
                    # Save WQ parameters table
                    output_csv_url = get_wq_csv_url(
                        output_directory=output_directory,
                        tile_id=tile_id,
                        temporal_id=temporal_id,
                        product_name=product_name,
                        product_version=product_version,
                    )
                    with fs.open(output_csv_url, mode="w") as f:
                        wq_vars_df.to_csv(f, index=False)
                    log.info(f"WQ parameters table saved to {output_csv_url}")
            # Generate STAC metadata
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)
                log.info("Creating metadata STAC file ...")
                stac_file_url = prepare_dataset(  # noqa F841
                    measurement_paths=measurement_paths,
                )

            log.info(f"Successfully processed task: {task_id_str}")

        except Exception as error:
            log.exception(error)
            failed_tasks.append(task_id)

    cache.close()

    # Handle failed tasks
    if failed_tasks:
        failed_tasks_json_array = json.dumps(failed_tasks)

        tasks_directory = "/tmp/"
        failed_tasks_output_file = join_url(tasks_directory, "failed_tasks")

        fs = get_filesystem(path=tasks_directory, anon=False)
        if not check_directory_exists(path=tasks_directory):
            fs.mkdirs(path=tasks_directory, exist_ok=True)

        with fs.open(failed_tasks_output_file, "a") as file:
            file.write(failed_tasks_json_array + "\n")
        log.error(f"Failed tasks: {failed_tasks_json_array}")
        log.info(f"Failed tasks written to {failed_tasks_output_file}")
        sys.exit(1)
    else:
        log.info(f"Worker {worker_idx} completed successfully!")
        sys.exit(0)


if __name__ == "__main__":
    cli()
